
# ============================================================================
# APEX YAML Configuration File
# ============================================================================
# Used by: CsvToH2PipelineTest.java
# Purpose: ETL pipeline demonstration for CSV to H2 database data loading
#          using APEX pipeline functionality for data extraction, transformation,
#          and loading operations with customer and trade data
# Status: Active test configuration - Organized by package structure
# ============================================================================

metadata:
  id: "csv-to-h2-pipeline-demo"
  name: "CSV to H2 ETL Pipeline Demo"
  version: "1.0.0"
  description: "Demonstration of CSV data processing with H2 database output using APEX data sinks"
  type: "pipeline-config"
  author: "APEX Demo Team"
  tags: ["demo", "etl", "csv", "h2", "pipeline"]

# Pipeline orchestration - defines the complete ETL workflow
pipeline:
  name: "customer-etl-pipeline"
  description: "Extract customer data from CSV, transform, and load into H2 database"

  # Pipeline steps executed in sequence
  steps:
    - name: "extract-customers"
      type: "extract"
      source: "customer-csv-input"
      operation: "getAllCustomers"
      description: "Read all customer records from CSV file"

    - name: "load-to-database"
      type: "load"
      sink: "customer-h2-database"
      operation: "insertCustomer"
      description: "Insert customer records into H2 database"
      depends-on: ["extract-customers"]

    - name: "audit-logging"
      type: "audit"
      sink: "audit-log-file"
      operation: "writeAuditRecord"
      description: "Write audit records to JSON file"
      depends-on: ["load-to-database"]
      optional: true

  # Pipeline execution configuration
  execution:
    mode: "sequential"
    error-handling: "stop-on-error"
    max-retries: 3
    retry-delay-ms: 1000

# Input data source configuration
data-sources:
  - name: "customer-csv-input"
    type: "file-system"
    enabled: true
    description: "Customer CSV file input for ETL processing"

    connection:
      base-path: "./target/demo/etl/data"
      file-pattern: "customers.csv"
      encoding: "UTF-8"

    operations:
      getAllCustomers: "SELECT * FROM csv"
      getActiveCustomers: "SELECT * FROM csv WHERE status = 'ACTIVE'"

# Output data sink configuration
data-sinks:
  - name: "customer-h2-database"
    type: "database"
    source-type: "h2"
    enabled: true
    description: "H2 database for storing processed customer data"

    connection:
      database: "./target/demo/etl/output/customer_database"
      username: "sa"
      password: ""

    schema:
      auto-create: true
      init-script: |
        CREATE TABLE IF NOT EXISTS customers (
          customer_id INTEGER PRIMARY KEY,
          customer_name VARCHAR(255) NOT NULL,
          email VARCHAR(255),
          status VARCHAR(50) DEFAULT 'ACTIVE',
          processed_at TIMESTAMP,
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );

    operations:
      insertCustomer: |
        INSERT INTO customers (
          customer_id, customer_name, email, status, processed_at, created_at
        ) VALUES (
          :column_1, :column_2, :column_3, :column_5, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
        )

  - name: "audit-log-file"
    type: "file-system"
    enabled: true
    description: "JSON file for audit logging of processed records"

    connection:
      base-path: "./target/demo/etl/output/audit"
      file-pattern: "customer_audit_{timestamp}.json"
      encoding: "UTF-8"

    operations:
      writeAuditRecord: "WRITE_JSON"
