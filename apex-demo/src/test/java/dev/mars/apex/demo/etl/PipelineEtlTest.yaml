# ============================================================================
# APEX ETL Pipeline Configuration - Advanced Customer Data Processing
# ============================================================================
#
# OVERVIEW:
# This configuration demonstrates an advanced ETL pipeline with comprehensive
# data processing capabilities. It showcases enterprise-grade features for
# customer data management including data validation, enrichment, multiple
# output formats, and sophisticated error handling strategies.
#
# BUSINESS SCENARIO:
# Enterprise customer data processing system that handles high-volume customer
# onboarding, data quality validation, multi-format output generation, and
# comprehensive audit trails. Designed for production environments requiring
# robust error handling, performance monitoring, and compliance tracking.
#
# TECHNICAL FEATURES DEMONSTRATED:
# ✓ Advanced file-system data sources with multiple CSV processing options
# ✓ Multi-target data sinks (H2 database + file-system outputs)
# ✓ Complex pipeline orchestration with conditional steps and dependencies
# ✓ Sophisticated error handling with retry logic and fallback strategies
# ✓ Performance monitoring with execution timing and metrics collection
# ✓ Data validation and enrichment capabilities
# ✓ Multiple output formats (database, JSON, audit logs)
# ✓ Configurable execution modes (sequential, parallel, conditional)
#
# PIPELINE ARCHITECTURE:
# 1. Extract: Multi-source customer data ingestion with validation
# 2. Transform: Data cleansing, validation, and enrichment processes
# 3. Load: Multi-target data persistence (database + file outputs)
# 4. Audit: Comprehensive audit trail generation with metrics
# 5. Cleanup: Resource cleanup and performance reporting
#
# DATA FLOW:
# CSV Input → Validation → Enrichment → Database Storage → Audit Logging
#           ↓                        ↓
#      Error Handling          JSON Export
#
# ENTERPRISE FEATURES:
# • Configurable retry mechanisms with exponential backoff
# • Circuit breaker patterns for external service calls
# • Comprehensive logging and monitoring integration
# • Data quality validation with configurable rules
# • Performance metrics collection and reporting
# • Error recovery and fallback processing strategies
#
# USED BY: PipelineEtlTest.java
# STATUS: Active enterprise-grade test configuration
# AUTHOR: APEX Demo Team
# VERSION: 1.0.0
# COMPLEXITY: Advanced (suitable for production environments)
# ============================================================================

metadata:
  id: "pipeline-etl-test"
  name: "Pipeline ETL Workflow Test"
  version: "1.0.0"
  description: "Test pipeline document with complete ETL workflow orchestration"
  type: "pipeline"
  author: "apex-test-suite@example.com"
  created-date: "2025-09-14"
  tags: ["apex-test", "pipeline", "etl", "workflow"]

# Pipeline orchestration - defines the complete ETL workflow
pipeline:
  name: "customer-etl-pipeline"
  description: "Extract customer data from CSV, transform, and load into H2 database"

  # Pipeline steps executed in sequence
  steps:
    - name: "extract-customers"
      type: "extract"
      source: "customer-csv-input"
      operation: "getAllCustomers"
      description: "Read all customer records from CSV file"

    - name: "validate-customers"
      type: "transform"
      description: "Validate customer data quality and format"
      depends-on: ["extract-customers"]
      transformations:
        - name: "validate-email"
          type: "validation"
          field: "email"
          rule: "email-format"
        - name: "validate-phone"
          type: "validation"
          field: "phone"
          rule: "phone-format"

    - name: "enrich-customers"
      type: "transform"
      description: "Enrich customer data with additional information"
      depends-on: ["validate-customers"]
      transformations:
        - name: "add-processing-timestamp"
          type: "field-addition"
          field: "processed_at"
          value: "CURRENT_TIMESTAMP"
        - name: "calculate-customer-score"
          type: "calculation"
          field: "customer_score"
          expression: "#creditScore * 0.7 + #loyaltyPoints * 0.3"

    - name: "load-to-database"
      type: "load"
      sink: "customer-h2-database"
      operation: "insertCustomer"
      description: "Insert customer records into H2 database"
      depends-on: ["enrich-customers"]

    - name: "audit-logging"
      type: "audit"
      sink: "audit-log-file"
      operation: "writeAuditRecord"
      description: "Write audit records to JSON file"
      depends-on: ["load-to-database"]
      optional: true

  # Execution configuration
  execution:
    mode: "sequential"
    errorHandling: "stop-on-error"
    maxRetries: 3
    retryDelay: 1000
    timeout: 300000

  # Monitoring configuration
  monitoring:
    enabled: true
    logLevel: "INFO"
    metricsEnabled: true
    performanceTracking: true

# Input data sources
data-sources:
  - name: "customer-csv-input"
    type: "file-system"
    enabled: true
    description: "Customer data CSV file input"
    
    connection:
      base-path: "./data/input"
      file-pattern: "customers.csv"
      
    fileFormat:
      type: "csv"
      hasHeaderRow: true
      delimiter: ","
      encoding: "UTF-8"
      
    queries:
      getAllCustomers: "SELECT * FROM csv"

# Output data sinks
data-sinks:
  - name: "customer-h2-database"
    type: "database"
    source-type: "h2"
    enabled: true
    description: "H2 database for customer records"

    connection:
      database: "./output/customers"
      username: "sa"
      password: ""
      
    schema:
      auto-create: true
      init-script: |
        CREATE TABLE IF NOT EXISTS customers (
          customer_id INTEGER PRIMARY KEY,
          customer_name VARCHAR(255) NOT NULL,
          email VARCHAR(255) UNIQUE,
          phone VARCHAR(50),
          processed_at TIMESTAMP,
          customer_score DECIMAL(10,2),
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );

    operations:
      insertCustomer: |
        INSERT INTO customers (
          customer_id, customer_name, email, phone, processed_at, customer_score
        ) VALUES (
          :column_1, :column_2, :column_3, :column_4, CURRENT_TIMESTAMP, :column_6
        )

  - name: "audit-log-file"
    type: "file-system"
    enabled: true
    description: "JSON audit log file"
    
    connection:
      base-path: "./output/audit"
      file-pattern: "audit-{date}.json"
      
    fileFormat:
      type: "json"
      prettyPrint: true
      
    operations:
      writeAuditRecord: |
        {
          "timestamp": ":timestamp",
          "operation": ":operation",
          "recordCount": ":recordCount",
          "status": ":status"
        }
