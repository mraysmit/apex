metadata:
  id: "pipeline-datasinks-test"
  name: "Pipeline Data Sinks Test"
  version: "1.0.0"
  description: "Test pipeline document with comprehensive data sinks configuration"
  type: "pipeline"
  author: "apex-test-suite@example.com"
  created-date: "2025-09-14"
  tags: ["apex-test", "pipeline", "data-sinks"]

# Pipeline orchestration
pipeline:
  name: "multi-sink-data-pipeline"
  description: "Pipeline distributing data to multiple output destinations"

  steps:
    - name: "extract-source-data"
      type: "extract"
      source: "input-data-source"
      operation: "getAllData"
      description: "Extract data from input source"

    - name: "transform-for-database"
      type: "transform"
      description: "Transform data for database storage"
      depends-on: ["extract-source-data"]

    - name: "transform-for-file"
      type: "transform"
      description: "Transform data for file output"
      depends-on: ["extract-source-data"]

    - name: "transform-for-api"
      type: "transform"
      description: "Transform data for API publishing"
      depends-on: ["extract-source-data"]

    - name: "load-to-database"
      type: "load"
      sink: "primary-database-sink"
      operation: "insertRecord"
      description: "Load data to primary database"
      depends-on: ["transform-for-database"]

    - name: "load-to-file"
      type: "load"
      sink: "json-file-sink"
      operation: "writeJsonRecord"
      description: "Write data to JSON file"
      depends-on: ["transform-for-file"]

    - name: "publish-to-api"
      type: "load"
      sink: "external-api-sink"
      operation: "publishData"
      description: "Publish data to external API"
      depends-on: ["transform-for-api"]
      optional: true

# Input data sources
data-sources:
  - name: "input-data-source"
    type: "database"
    sourceType: "h2"
    enabled: true
    description: "Input data source"
    
    connection:
      database: "./input/source_data"
      username: "sa"
      password: ""
      
    queries:
      getAllData: "SELECT * FROM source_table ORDER BY created_at"

# Output data sinks - comprehensive configuration
data-sinks:
  - name: "primary-database-sink"
    type: "database"
    sourceType: "postgresql"
    enabled: true
    description: "Primary PostgreSQL database for processed data"
    
    connection:
      host: "localhost"
      port: 5432
      database: "processed_data"
      username: "app_user"
      password: "${DB_PASSWORD}"
      
    operations:
      insertRecord: |
        INSERT INTO processed_records (record_id, data_json, processed_at, status) 
        VALUES (:record_id, :data_json, CURRENT_TIMESTAMP, 'PROCESSED')
      updateRecord: |
        UPDATE processed_records 
        SET data_json = :data_json, updated_at = CURRENT_TIMESTAMP 
        WHERE record_id = :record_id
      deleteRecord: |
        DELETE FROM processed_records WHERE record_id = :record_id
        
    schema:
      autoCreate: true
      init-script: |
        CREATE TABLE IF NOT EXISTS processed_records (
          record_id VARCHAR(255) PRIMARY KEY,
          data_json JSONB NOT NULL,
          processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          status VARCHAR(50) DEFAULT 'PENDING'
        );
        CREATE INDEX IF NOT EXISTS idx_processed_records_status ON processed_records(status);
        CREATE INDEX IF NOT EXISTS idx_processed_records_processed_at ON processed_records(processed_at);
        
    performance:
      batchSize: 1000
      connectionPoolSize: 10
      queryTimeout: 30000

  - name: "json-file-sink"
    type: "file-system"
    enabled: true
    description: "JSON file output for data archival"
    
    connection:
      basePath: "./output/archive"
      filePattern: "processed-data-{date}.json"
      createDirectories: true
      
    fileFormat:
      type: "json"
      prettyPrint: true
      encoding: "UTF-8"
      
    operations:
      writeJsonRecord: |
        {
          "recordId": ":record_id",
          "data": :data_json,
          "processedAt": ":processed_at",
          "metadata": {
            "pipeline": "multi-sink-data-pipeline",
            "version": "1.0.0"
          }
        }
      writeJsonBatch: |
        {
          "batchId": ":batch_id",
          "records": :records_json,
          "processedAt": ":processed_at",
          "recordCount": :record_count
        }
        
    rotation:
      enabled: true
      maxFileSize: "100MB"
      maxFiles: 30
      
    compression:
      enabled: true
      algorithm: "gzip"

  - name: "csv-file-sink"
    type: "file-system"
    enabled: true
    description: "CSV file output for reporting"
    
    connection:
      basePath: "./output/reports"
      filePattern: "daily-report-{date}.csv"
      
    fileFormat:
      type: "csv"
      hasHeaderRow: true
      delimiter: ","
      quoteChar: "\""
      encoding: "UTF-8"
      
    operations:
      writeCsvRecord: "record_id,data_summary,processed_at"
      
    schema:
      columns:
        - name: "record_id"
          type: "string"
          required: true
        - name: "data_summary"
          type: "string"
          required: false
        - name: "processed_at"
          type: "timestamp"
          required: true

  - name: "external-api-sink"
    type: "rest-api"
    enabled: true
    description: "External API for data publishing"
    
    connection:
      baseUrl: "https://api.external-system.com/v1"
      timeout: 15000
      maxConnections: 3
      
    authentication:
      type: "bearer"
      token: "${EXTERNAL_API_TOKEN}"
      
    endpoints:
      publishData: "/data/publish"
      publishBatch: "/data/batch"
      
    operations:
      publishData: |
        POST /data/publish
        Content-Type: application/json
        {
          "recordId": ":record_id",
          "data": :data_json,
          "source": "apex-pipeline"
        }
        
    retryPolicy:
      maxRetries: 3
      backoffStrategy: "exponential"
      initialDelayMs: 1000
      maxDelayMs: 10000
      
    circuitBreaker:
      enabled: true
      failureThreshold: 5
      recoveryTimeoutMs: 60000
