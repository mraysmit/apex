# APEX ETL Demo Configuration
# CSV to H2 Database Pipeline Example
#
# This configuration demonstrates a complete ETL pipeline using APEX data sinks
# to process CSV customer data and write it to an H2 database.

metadata:
  id: "csv-to-h2-pipeline-demo"
  name: "CSV to H2 ETL Pipeline Demo"
  version: "1.0.0"
  description: "Demonstration of CSV data processing with H2 database output using APEX data sinks"
  type: "pipeline"
  author: "APEX Demo Team"
  tags: ["demo", "etl", "csv", "h2", "pipeline"]

# Pipeline orchestration - defines the complete ETL workflow
pipeline:
  name: "customer-etl-pipeline"
  description: "Extract customer data from CSV, transform, and load into H2 database"

  # Pipeline steps executed in sequence
  steps:
    - name: "extract-customers"
      type: "extract"
      source: "customer-csv-input"
      operation: "getAllCustomers"
      description: "Read all customer records from CSV file"

    - name: "load-to-database"
      type: "load"
      sink: "customer-h2-database"
      operation: "insertCustomer"
      description: "Insert customer records into H2 database"
      depends-on: ["extract-customers"]

    - name: "audit-logging"
      type: "audit"
      sink: "audit-log-file"
      operation: "writeAuditRecord"
      description: "Write audit records to JSON file"
      depends-on: ["load-to-database"]
      optional: true

  # Pipeline execution configuration
  execution:
    mode: "sequential"  # or "parallel" for independent steps
    error-handling: "stop-on-error"  # or "continue-on-error"
    max-retries: 3
    retry-delay-ms: 1000

  # Pipeline-level transformations (optional)
  transformations:
    - name: "add-processing-timestamp"
      type: "field-addition"
      field: "processed_at"
      value: "CURRENT_TIMESTAMP"

    - name: "validate-email"
      type: "validation"
      field: "email"
      rule: "email-format"

  # Pipeline monitoring and metrics
  monitoring:
    enabled: true
    log-progress: true
    collect-metrics: true
    alert-on-failure: true

# Input data source configuration
data-sources:
  - name: "customer-csv-input"
    type: "file-system"
    source-type: "csv"
    enabled: true
    description: "Customer CSV file input for ETL processing"
    tags: ["csv", "input", "customers"]

    # File system connection
    connection:
      base-path: "./target/demo/etl/data"
      file-pattern: "customers.csv"
      encoding: "UTF-8"
      recursive: false
      watch-for-changes: false

    # CSV format configuration
    file-format:
      type: "csv"
      has-header-row: true
      delimiter: ","
      quote-character: "\""
      escape-character: "\\"
      skip-lines: 0
      encoding: "UTF-8"

      # Column mappings from CSV headers to internal field names
      column-mappings:
        "customer_id": "id"
        "customer_name": "customerName"
        "email_address": "email"
        "registration_date": "registrationDate"
        "status": "status"

      # Data type conversions
      column-types:
        "id": "integer"
        "customerName": "string"
        "email": "string"
        "registrationDate": "date"
        "status": "string"

      # Date format for parsing registration_date
      date-format: "yyyy-MM-dd"

    # Query operations for CSV data
    operations:
      getAllCustomers: "SELECT * FROM csv"
      getActiveCustomers: "SELECT * FROM csv WHERE status = 'ACTIVE'"
      getCustomerById: "SELECT * FROM csv WHERE id = :id"

    # Parameter names for operations
    parameter-names: ["id", "status"]

    # Health check configuration
    health-check:
      enabled: true
      interval-seconds: 60
      timeout-seconds: 5

# Output data sink configuration
data-sinks:
  - name: "customer-h2-database"
    type: "database"
    source-type: "h2"
    enabled: true
    description: "H2 database for storing processed customer data"
    tags: ["database", "h2", "customers"]
    
    # Database connection configuration
    connection:
      database: "./target/demo/etl/output/customer_database"
      username: "sa"
      password: ""
      mode: "PostgreSQL"
      
      # Connection pool settings for better performance
      connection-pool:
        max-size: 10
        min-size: 2
        connection-timeout: 30000
        idle-timeout: 600000
        max-lifetime: 1800000
    
    # Database operations (SQL statements)
    operations:
      # Insert new customer record
      insertCustomer: |
        INSERT INTO customers (
          customer_id,
          customer_name,
          email,
          status,
          processed_at,
          created_at
        ) VALUES (
          :id,
          :customerName,
          :email,
          :status,
          CURRENT_TIMESTAMP,
          CURRENT_TIMESTAMP
        )
      
      # Update existing customer record
      updateCustomer: |
        UPDATE customers 
        SET customer_name = :customerName, 
            email = :email, 
            status = :status,
            processed_at = :processedAt,
            updated_at = CURRENT_TIMESTAMP
        WHERE customer_id = :id
      
      # Upsert customer record (insert or update)
      upsertCustomer: |
        MERGE INTO customers (
          customer_id, 
          customer_name, 
          email, 
          status, 
          processed_at,
          created_at,
          updated_at
        ) KEY (customer_id) VALUES (
          :id, 
          :customerName, 
          :email, 
          :status, 
          :processedAt,
          CURRENT_TIMESTAMP,
          CURRENT_TIMESTAMP
        )
      
      # Select customer by ID (for verification)
      selectCustomer: |
        SELECT customer_id, customer_name, email, status, processed_at, created_at, updated_at
        FROM customers 
        WHERE customer_id = :id
      
      # Count total customers
      countCustomers: |
        SELECT COUNT(*) as total_customers FROM customers
    
    # Schema management configuration
    schema:
      auto-create: true
      table-name: "customers"
      schema-name: "public"
      
      # Database initialization script
      init-script: |
        -- Create customers table if it doesn't exist
        CREATE TABLE IF NOT EXISTS customers (
          customer_id INTEGER PRIMARY KEY,
          customer_name VARCHAR(255) NOT NULL,
          email VARCHAR(255) UNIQUE,
          status VARCHAR(50) DEFAULT 'ACTIVE',
          processed_at TIMESTAMP,
          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
          updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- Create index on email for faster lookups
        CREATE INDEX IF NOT EXISTS idx_customers_email ON customers(email);
        
        -- Create index on status for filtering
        CREATE INDEX IF NOT EXISTS idx_customers_status ON customers(status);
    
    # Error handling configuration
    error-handling:
      strategy: "log-and-continue"
      max-retries: 3
      retry-delay: 1000
      retry-backoff-multiplier: 2.0
      max-retry-delay: 10000
      
      # Dead letter handling for failed records
      dead-letter-enabled: true
      dead-letter-table: "failed_customer_records"
      
      # Logging configuration
      log-errors: true
      log-level: "WARN"
      include-stack-trace: false
      include-data: false
      max-logged-errors: 100
    
    # Batch processing configuration
    batch:
      enabled: true
      mode: "size-based"
      batch-size: 50
      max-batch-size: 200
      min-batch-size: 1
      timeout-ms: 10000
      max-timeout-ms: 30000
      
      # Transaction management
      transaction-mode: "per-batch"
      transaction-timeout-ms: 30000
      isolation-level: "READ_COMMITTED"
      
      # Memory management
      max-memory-usage-mb: 100
      enable-memory-monitoring: true
      memory-threshold-percent: 0.8
      
      # Performance tuning
      parallel-batches: 1
      enable-compression: false
      maintain-order: true
      
      # Monitoring
      enable-metrics: true
      log-batch-statistics: true
      metrics-reporting-interval-ms: 10000
    
    # Health check configuration
    health-check:
      enabled: true
      interval-seconds: 30
      timeout-seconds: 5
      query: "SELECT 1"
      
    # Circuit breaker configuration
    circuit-breaker:
      enabled: true
      failure-threshold: 5
      timeout-seconds: 60
      success-threshold: 3
    
    # Custom properties for demo
    custom-properties:
      demo-mode: true
      demo-version: "1.0.0"
      demo-description: "CSV to H2 ETL Pipeline Demo"
      
    # Parameter names for operations
    parameter-names: ["id", "customerName", "email", "status"]

# Additional data sink for audit logging (file-based)
  - name: "audit-log-file"
    type: "file-system"
    source-type: "json"
    enabled: true
    description: "JSON file for audit logging of processed records"
    tags: ["audit", "logging", "json"]
    
    # File system connection
    connection:
      base-path: "./target/demo/etl/output/audit"
      file-pattern: "customer_audit_{timestamp}.json"
      encoding: "UTF-8"
      
    # File operations
    operations:
      writeAuditRecord: "WRITE_JSON"
      appendAuditRecord: "APPEND_JSON"
    
    # Output format configuration
    output-format:
      format: "json"
      pretty-print: true
      encoding: "UTF-8"
      include-timestamp: true
      date-format: "yyyy-MM-dd'T'HH:mm:ss.SSSZ"
    
    # Batch configuration for file output
    batch:
      enabled: true
      batch-size: 100
      flush-interval-ms: 5000
      enable-buffering: true
      buffer-size: 1000
    
    # Error handling for file operations
    error-handling:
      strategy: "log-and-continue"
      max-retries: 2
      retry-delay: 500
